# PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving

A framework for fine-tuning large language models to generate step-by-step solution plans and execute them on mathematical benchmarks (GSM8K, MATH), including out-of-distribution evaluations. Plan-Tuning improves LLM reasoning by first fine-tuning on "plans"â€”structured, step-by-step outlines of problem-solving strategiesâ€”before generating final answers.

## ğŸ—‚ Project Structure

```
.
â”œâ”€â”€ README.md                â† this file
â”œâ”€â”€ requirements.txt         â† dependencies
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ math/
â”‚   â”‚   â”œâ”€â”€ processed_train_method_1.json              â† training file for method 1 in the paper
â”‚   â”‚   â”œâ”€â”€ processed_train_method_2.json              â† training file for method 2 in the paper
â”‚   â”‚   â”œâ”€â”€ processed_train_sft.json                   â† training file for baseline SFT in the paper
â”‚   â”‚   â””â”€â”€ processed_test_sft_and_methods.json        â† test file for baseline SFT and proposed methods in the paper
â”‚   â”œâ”€â”€ gsm8k/
â”‚   â”‚   â”œâ”€â”€ processed_train_method_1.json
â”‚   â”‚   â”œâ”€â”€ processed_train_method_2.json
â”‚   â”‚   â”œâ”€â”€ processed_train_sft.json
â”‚   â”‚   â”œâ”€â”€ processed_test_sft.json                    â† test file for baseline SFT in the paper
â”‚   â”‚   â””â”€â”€ processed_test_methods.json                â† test file for proposed methods in the paper
â”‚   â””â”€â”€ test_ood/
â”‚       â”œâ”€â”€ aime/
â”‚       â”‚   â”œâ”€â”€ processed_test_zero_cot.json           â† test file for baseline zero-shot CoT in the paper
â”‚       â”‚   â”œâ”€â”€ processed_test_method_2_gsm8k.json     â† test file for method 2 in the paper
â”‚       â”‚   â”œâ”€â”€ processed_test_method_2_math.json      â† test file for method 2 in the paper
â”‚       â”‚   â””â”€â”€ processed_test_sft_and_methods.json    â† test file for baseline SFT and proposed methods in the paper
â”‚       â””â”€â”€ olympiad/
â”‚           â”œâ”€â”€ processed_test_zero_cot.json
â”‚           â”œâ”€â”€ processed_test_method_2_gsm8k.json
â”‚           â”œâ”€â”€ processed_test_method_2_math.json
â”‚           â””â”€â”€ processed_test_sft_and_methods.json
â””â”€â”€ src/
    â”œâ”€â”€ sft_training/
    â”‚   â”œâ”€â”€ run_sft.sh           â† launch script for SFT
    â”‚   â”œâ”€â”€ run_sft.py           â† entry-point for training
    â”‚   â”œâ”€â”€ sft_trainer.py       â† custom trainer implementation
    â”‚   â”œâ”€â”€ data_collator.py     â† batch collation logic
    â”‚   â”œâ”€â”€ trainer.py           â† wrapper around HF Trainer
    â”‚   â””â”€â”€ utils.py             â† data & helper functions
    â”œâ”€â”€ best_of_n/
    â”‚   â”œâ”€â”€ llm.py               â† LLM interface & helpers
    â”‚   â”œâ”€â”€ solve_gsm8k.py       â† best-of-n for GSM8K
    â”‚   â”œâ”€â”€ solve_math.py        â† best-of-n for MATH
    â”‚   â”œâ”€â”€ solve_test.py        â† inference on test splits
    â”‚   â””â”€â”€ self_consistency.py  â† self-consistency decoding
    â””â”€â”€ sft_eval/
        â”œâ”€â”€ vllm_hf_eval.py      â† evaluation via vLLM/HF
        â””â”€â”€ run_eval.sh          â† evaluation launcher
```

## âš™ï¸ Installation

### Clone the repo

```bash
git clone https://github.com/your-org/plan-tuning.git
cd plan-tuning
```

### Create a Python 3.8+ environment & install dependencies

```bash
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

**requirements.txt example:**

```
torch
transformers
datasets
trl
accelerate
wandb
pandas
tqdm
```

## ğŸ“¦ Data Preparation

### Combined GSM8K + MATH

Use `data/both_math_and_gsm8k/processed_train_*.json` for multi-domain training.

### GSM8K & MATH

Process raw datasets into `data/gsm8k/` and `data/math/` folders.

### Out-of-Distribution (OOD) Splits

Evaluate on AIME & Olympiad: `data/test_ood/{aime,olympiad}/`.

*(Add or update scripts in `src/data_processing.py` as needed.)*

## ğŸ“ Supervised Fine-Tuning (Plan-Tuning)

Train an LLM on plan-annotated examples:

```bash
cd src/sft_training
bash run_sft.sh \
  --data_path ../../data/both_math_and_gsm8k/processed_train_sft.json \
  --model_name_or_path "Qwen/Qwen3-8B" \
  --output_dir   ../models/plan_tuned_combo \
  --name          plan_tuned_combo \
  --batch_size    2 \
  --gradient_accumulation_steps 4 \
  --num_train_epochs    3 \
  --max_seq_len    4096 \
  --wandb_project  plan_tuning
```

Use `run_sft.py --help` for full flag list.

## ğŸ“ Plan Generation

Generate plans & answers:

### Best-of-n:

```bash
python src/best_of_n/solve_gsm8k.py \
  --model_path ../models/plan_tuned_combo \
  --output_dir  ../outputs/gsm8k_bestof5 \
  --num_samples 5
```

### Self-Consistency:

```bash
python src/best_of_n/self_consistency.py \
  --input  ../outputs/gsm8k_bestof5/plans.json \
  --output ../outputs/gsm8k_sc.json
```

## ğŸ§® Evaluation

Evaluate generated outputs:

```bash
cd src/sft_eval
bash run_eval.sh \
  --predictions ../../outputs/gsm8k_sc.json \
  --references ../../data/gsm8k/processed_test_methods.json \
  --output    ../eval_results/gsm8k_metrics.json
```

Or directly:

```bash
python vllm_hf_eval.py \
  --pred_file ../../outputs/gsm8k_sc.json \
  --ref_file  ../../data/gsm8k/processed_test_methods.json
```

## ğŸ“Š Results & Logging

- Logs & metrics tracked with Weights & Biases (set `--wandb_project`).
- Local logs: `src/sft_training/wandb/`, `src/sft_eval/logs/`.

## ğŸ¤ Contributing

1. Fork & clone
2. Create a branch: `git checkout -b feature/your_feature`
3. Commit & push
4. Open a PR

Please adhere to existing code style and add tests/examples for new features.

## ğŸ“„ License

This project is licensed under the Apache 2.0 License. See LICENSE for details.

## ğŸ“– Citation

If you use this code, please cite:

```bibtex
@inproceedings{parmar2025plan,
  title     = {Plan-Tuning: Fine-Tuning Language Models via Step-by-Step Planning},
  author    = {Parmar, Mihir and â€¦},
  booktitle = {NeurIPS 2025},
  year      = {2025}
}
```

Happy plan-tuning! ğŸš€
