# PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving

A framework for fine-tuning large language models to generate step-by-step solution plans and execute them on mathematical benchmarks (GSM8K, MATH), including out-of-distribution evaluations. Plan-Tuning improves LLM reasoning by first fine-tuning on "plans"â€”structured, step-by-step outlines of problem-solving strategiesâ€”before generating final answers.

## ğŸ—‚ Repository Structure

```
.
â”œâ”€â”€ README.md                â† this file
â”œâ”€â”€ requirements.txt         â† dependencies
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ math/
â”‚   â”‚   â”œâ”€â”€ processed_train_method_1.json              â† training file for method 1 in the paper
â”‚   â”‚   â”œâ”€â”€ processed_train_method_2.json              â† training file for method 2 in the paper
â”‚   â”‚   â”œâ”€â”€ processed_train_sft.json                   â† training file for baseline SFT in the paper
â”‚   â”‚   â””â”€â”€ processed_test_sft.json                    â† test file for baseline SFT and proposed methods in the paper
â”‚   â”‚   â””â”€â”€ processed_test_method_2.json               â† test file for method 2 in the paper
â”‚   â”œâ”€â”€ gsm8k/
â”‚   â”‚   â”œâ”€â”€ processed_train_method_1.json
â”‚   â”‚   â”œâ”€â”€ processed_train_method_2.json
â”‚   â”‚   â”œâ”€â”€ processed_train_sft.json
â”‚   â”‚   â”œâ”€â”€ processed_test_sft.json                    â† test file for baseline SFT in the paper
â”‚   â”‚   â””â”€â”€ processed_test_methods.json                â† test file for proposed methods in the paper
â”‚   â””â”€â”€ test_ood/
â”‚       â”œâ”€â”€ aime/
â”‚       â”‚   â”œâ”€â”€ processed_test_zero_cot.json           â† test file for baseline zero-shot CoT in the paper
â”‚       â”‚   â”œâ”€â”€ processed_test_method_2_gsm8k.json     â† test file for method 2 in the paper for GSM8k trained models
â”‚       â”‚   â”œâ”€â”€ processed_test_method_2_math.json      â† test file for method 2 in the paper for MATH trained models
â”‚       â”‚   â””â”€â”€ processed_test_sft.json                â† test file for baseline SFT and proposed methods in the paper
â”‚       â””â”€â”€ olympiad/
â”‚           â”œâ”€â”€ processed_test_zero_cot.json
â”‚           â”œâ”€â”€ processed_test_method_2_gsm8k.json
â”‚           â”œâ”€â”€ processed_test_method_2_math.json
â”‚           â””â”€â”€ processed_test_sft.json
â””â”€â”€ src/
    â”œâ”€â”€ sft_training/
    â”‚   â”œâ”€â”€ run_sft.sh           â† launch script for SFT
    â”‚   â”œâ”€â”€ run_sft.py           â† entry-point for training
    â”‚   â”œâ”€â”€ sft_trainer.py       â† custom trainer implementation
    â”‚   â”œâ”€â”€ data_collator.py     â† batch collation logic
    â”‚   â”œâ”€â”€ trainer.py           â† wrapper around HF Trainer
    â”‚   â””â”€â”€ utils.py             â† data & helper functions
    â”œâ”€â”€ best_of_n/
    â”‚   â”œâ”€â”€ llm.py               â† LLM interface & helpers
    â”‚   â”œâ”€â”€ solve_gsm8k.py       â† best-of-n for GSM8K
    â”‚   â”œâ”€â”€ solve_math.py        â† best-of-n for MATH
    â”‚   â”œâ”€â”€ solve_test.py        â† inference on test splits
    â”‚   â””â”€â”€ self_consistency.py  â† self-consistency decoding
    â””â”€â”€ sft_eval/
        â”œâ”€â”€ vllm_hf_eval.py      â† evaluation via vLLM/HF
        â””â”€â”€ run_eval.sh          â† evaluation launcher
```

## âš™ï¸ Installation

### Clone the repo

```bash
git clone https://github.com/Mihir3009/plan-tuning.git
cd plan-tuning
```

### Create a Python 3.9+ environment & install dependencies

```bash
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

## ğŸ“¦ Data Preparation

### GSM8K & MATH

Process raw datasets into `data/gsm8k/` and `data/math/` folders. We already provided processed files for our paper in these folders.

### Out-of-Distribution (OOD) Splits

Evaluate on AIME & OlympiadBench: `data/test_ood/{aime,olympiad}/`.

## ğŸ“ Supervised Fine-Tuning (Plan-Tuning)

Train an LLM on plan-annotated examples:

```bash
cd src/sft_training
bash run_sft.sh \
  --data_path ../../data/both_math_and_gsm8k/processed_train_sft.json \
  --model_path "Qwen/Qwen3-8B" [huggingface path or trained model path] \
  --output_path   [output directory] \
  --name          [name of folder for directory] \
  --batch_size    2 \
  --gradient_accumulation_steps 4 \
  --num_train_epochs    3 \
  --max_seq_len    4096 \
  --wandb_project  [project name]
```

## ğŸ“ Plan Generation

Generate plans & answers:

### Best-of-n:

```bash
python src/best_of_n/solve_gsm8k.py \ [change paths in the file]
```

- Do the same for the MATH dataset.

## ğŸ§® Evaluation

Evaluate generated outputs:

```bash
python vllm_hf_eval.py \
    --model_path [huggingface path or trained model path] \
    --test_path ../../data/both_math_and_gsm8k/processed_test_sft.json \
    --output_path [output directory]
```

## ğŸ“„ License

This project is licensed under the Apache 2.0 License. See LICENSE for details.

Happy plan-tuning! ğŸš€
